{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0W5lY9w5tQuiH4kYsoVMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter04/01_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OwnwBCe3rK6",
        "colab_type": "text"
      },
      "source": [
        "# **Cartpole game - обучение с подкреплением методом кросс-энтропии**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjagfJ5q5qVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "22021628-2310-4073-9049-330344b175a3"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TfrDCF1BhTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5rbllv_1qRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
        "\n",
        "\n",
        "def iterate_batches(env, net, batch_size):\n",
        "    batch = []\n",
        "    episode_reward = 0.0\n",
        "    episode_steps = []\n",
        "    obs = env.reset()\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    while True:\n",
        "        obs_v = torch.FloatTensor([obs])\n",
        "        act_probs_v = sm(net(obs_v))\n",
        "        act_probs = act_probs_v.data.numpy()[0]\n",
        "        action = np.random.choice(len(act_probs), p=act_probs)\n",
        "        next_obs, reward, is_done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
        "        if is_done:\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "            episode_reward = 0.0\n",
        "            episode_steps = []\n",
        "            next_obs = env.reset()\n",
        "            if len(batch) == batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        obs = next_obs\n",
        "\n",
        "\n",
        "def filter_batch(batch, percentile):\n",
        "    rewards = list(map(lambda s: s.reward, batch))\n",
        "    reward_bound = np.percentile(rewards, percentile)\n",
        "    reward_mean = float(np.mean(rewards))\n",
        "\n",
        "    train_obs = []\n",
        "    train_act = []\n",
        "    for example in batch:\n",
        "        if example.reward < reward_bound:\n",
        "            continue\n",
        "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "        train_act.extend(map(lambda step: step.action, example.steps))\n",
        "\n",
        "    train_obs_v = torch.FloatTensor(train_obs)\n",
        "    train_act_v = torch.LongTensor(train_act)\n",
        "    return train_obs_v, train_act_v, reward_bound, reward_mean"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJFYe-yJBqwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "outputId": "e73c1390-c6fd-4dcf-d85a-cdcb3e8615a2"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
        "    obs_size = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "    writer = SummaryWriter(comment=\"-cartpole\")\n",
        "\n",
        "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
        "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
        "        optimizer.zero_grad()\n",
        "        action_scores_v = net(obs_v)\n",
        "        loss_v = objective(action_scores_v, acts_v)\n",
        "        loss_v.backward()\n",
        "        optimizer.step()\n",
        "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
        "            iter_no, loss_v.item(), reward_m, reward_b))\n",
        "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "        if reward_m > 199:\n",
        "            print(\"Solved!\")\n",
        "            break\n",
        "    writer.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=0.698, reward_mean=18.1, reward_bound=20.0\n",
            "1: loss=0.686, reward_mean=21.4, reward_bound=24.0\n",
            "2: loss=0.675, reward_mean=29.0, reward_bound=34.0\n",
            "3: loss=0.661, reward_mean=33.6, reward_bound=40.0\n",
            "4: loss=0.646, reward_mean=24.9, reward_bound=27.5\n",
            "5: loss=0.644, reward_mean=31.2, reward_bound=34.5\n",
            "6: loss=0.629, reward_mean=37.9, reward_bound=42.0\n",
            "7: loss=0.645, reward_mean=46.9, reward_bound=52.0\n",
            "8: loss=0.605, reward_mean=56.4, reward_bound=61.5\n",
            "9: loss=0.626, reward_mean=46.6, reward_bound=56.5\n",
            "10: loss=0.596, reward_mean=43.6, reward_bound=46.5\n",
            "11: loss=0.591, reward_mean=50.2, reward_bound=53.5\n",
            "12: loss=0.606, reward_mean=38.2, reward_bound=46.0\n",
            "13: loss=0.587, reward_mean=55.8, reward_bound=62.5\n",
            "14: loss=0.574, reward_mean=49.2, reward_bound=43.0\n",
            "15: loss=0.572, reward_mean=55.4, reward_bound=54.5\n",
            "16: loss=0.580, reward_mean=42.5, reward_bound=47.0\n",
            "17: loss=0.588, reward_mean=59.7, reward_bound=72.5\n",
            "18: loss=0.584, reward_mean=55.3, reward_bound=59.5\n",
            "19: loss=0.557, reward_mean=58.5, reward_bound=70.0\n",
            "20: loss=0.571, reward_mean=74.9, reward_bound=74.0\n",
            "21: loss=0.550, reward_mean=55.1, reward_bound=69.5\n",
            "22: loss=0.563, reward_mean=67.4, reward_bound=70.0\n",
            "23: loss=0.549, reward_mean=61.5, reward_bound=79.5\n",
            "24: loss=0.538, reward_mean=64.5, reward_bound=66.0\n",
            "25: loss=0.534, reward_mean=69.5, reward_bound=84.5\n",
            "26: loss=0.551, reward_mean=74.8, reward_bound=74.5\n",
            "27: loss=0.546, reward_mean=75.9, reward_bound=88.5\n",
            "28: loss=0.544, reward_mean=89.6, reward_bound=90.0\n",
            "29: loss=0.523, reward_mean=96.6, reward_bound=102.5\n",
            "30: loss=0.532, reward_mean=103.9, reward_bound=103.0\n",
            "31: loss=0.529, reward_mean=104.4, reward_bound=116.0\n",
            "32: loss=0.554, reward_mean=151.7, reward_bound=178.0\n",
            "33: loss=0.538, reward_mean=120.2, reward_bound=140.0\n",
            "34: loss=0.541, reward_mean=143.3, reward_bound=185.0\n",
            "35: loss=0.539, reward_mean=163.9, reward_bound=200.0\n",
            "36: loss=0.531, reward_mean=159.8, reward_bound=200.0\n",
            "37: loss=0.521, reward_mean=144.4, reward_bound=168.0\n",
            "38: loss=0.529, reward_mean=152.0, reward_bound=177.0\n",
            "39: loss=0.529, reward_mean=145.9, reward_bound=185.0\n",
            "40: loss=0.524, reward_mean=164.6, reward_bound=188.0\n",
            "41: loss=0.530, reward_mean=177.2, reward_bound=200.0\n",
            "42: loss=0.531, reward_mean=181.8, reward_bound=200.0\n",
            "43: loss=0.536, reward_mean=192.2, reward_bound=200.0\n",
            "44: loss=0.526, reward_mean=190.9, reward_bound=200.0\n",
            "45: loss=0.531, reward_mean=191.7, reward_bound=200.0\n",
            "46: loss=0.535, reward_mean=200.0, reward_bound=200.0\n",
            "Solved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i983yj46ADX",
        "colab_type": "text"
      },
      "source": [
        "# **Connert to tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77KoHI-W6BWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_CoCsxv6FFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "38155fa2-536a-4371-f316-012ad6c2fa67"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}